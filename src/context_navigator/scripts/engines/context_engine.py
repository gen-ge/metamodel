#!/usr/bin/env python3

# ===== CONTEXT NAVIGATOR CODE BRIDGE =====
# @cn:component context-engine
# @cn:doc context-engine.md
# @cn:context-level c3_component
# @cn:context-type core
# @cn:parent-module cli-interface
# @cn:purpose "Motor principal de processamento contextual - automatiza decis√µes sobre templates e contextos"
# @cn:memory-aid "Cora√ß√£o do sistema - analisa conte√∫do, recomenda templates, sugere conex√µes e melhorias"
# @cn:depends-on context.rule, CONVENTIONS.md, template-definitions
# @cn:impacts template-validation, document-processing, user-guidance
# @cn:provides content-analysis, template-recommendation, context-suggestion, improvement-suggestions
# @cn:component-type functional
# @cn:responsibility context-processing
# @cn:single-purpose true
# @cn:complexity critical
# @cn:owner Context Navigator Team
# @cn:last-updated 2025-01-13
# ============================================

"""
Context Navigator - Context Engine
Engine metodol√≥gica inteligente que automatiza decis√µes sobre templates, 
contextos e sugere melhorias baseado na an√°lise de documentos.
"""

import os
import sys
import yaml
import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import logging

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('context_engine')

# @cn:class business-entity
# @cn:responsibility data-structure
# @cn:purpose "Estrutura de dados para resultado de an√°lise de conte√∫do"
@dataclass
class ContentAnalysis:
    """Resultado da an√°lise de conte√∫do"""
    keywords: List[str]
    entities: List[str]
    complexity_score: float
    purpose: str
    domain: str
    confidence: float

# @cn:class business-entity
# @cn:responsibility data-structure
# @cn:purpose "Estrutura de dados para recomenda√ß√£o de template"
@dataclass
class TemplateRecommendation:
    """Recomenda√ß√£o de template"""
    template_type: str
    confidence: float
    reasoning: str
    alternative_templates: List[str]

# @cn:class business-entity
# @cn:responsibility data-structure
# @cn:purpose "Estrutura de dados para recomenda√ß√£o de contexto"
@dataclass
class ContextRecommendation:
    """Recomenda√ß√£o de contexto"""
    context_level: str
    context_type: str
    confidence: float
    reasoning: str
    suggested_module: str

@dataclass
class ConnectionSuggestion:
    """Sugest√£o de conex√£o"""
    target_document: str
    connection_type: str
    confidence: float
    reasoning: str

@dataclass
class ImprovementSuggestion:
    """Sugest√£o de melhoria"""
    type: str
    description: str
    priority: str
    action: str

# @cn:class service
# @cn:responsibility context-processing
# @cn:pattern singleton
# @cn:lifecycle singleton
# @cn:purpose "Servi√ßo principal que analisa conte√∫do e recomenda templates/contextos"
class ContextEngine:
    """Engine metodol√≥gica inteligente do Context Navigator"""
    
    # @cn:function core
    # @cn:process initialization
    # @cn:step 1
    def __init__(self, base_path: str = "."):
        """
        Inicializa a engine
        
        Args:
            base_path: Caminho base do projeto
        """
        self.base_path = Path(base_path)
        self.config = {}
        self.documents = {}
        self.context_maps = {}
        
        # NOVO: Usar WorkspaceManager para detectar workspace
        self._init_with_workspace_manager()
        
        # Carregar dados de contexto
        self._load_context_maps()
        
        # Padr√µes para an√°lise de conte√∫do
        self._init_content_patterns()
        
    def _init_with_workspace_manager(self):
        """Inicializa usando WorkspaceManager para detectar workspace"""
        # Importar WorkspaceManager
        try:
            from ..core.workspace_manager import WorkspaceManager, Workspace
        except ImportError:
            # Fallback para desenvolvimento
            import sys
            sys.path.insert(0, str(Path(__file__).parent.parent))
            from core.workspace_manager import WorkspaceManager, Workspace
        
        # Detectar workspace atual
        workspace_manager = WorkspaceManager()
        current_workspace = workspace_manager.detect_current_workspace()
        
        if not current_workspace:
            logger.error("‚ùå Context Navigator workspace n√£o encontrado")
            logger.error("üí° Execute 'cn init' para configurar este diret√≥rio")
            sys.exit(1)
        
        # Configurar paths baseado no workspace
        self.workspace = current_workspace
        self.base_path = current_workspace.root_path
        self.output_dir = current_workspace.root_path / ".cn_model"
        
        # Configura√ß√£o vem do workspace
        self.config = current_workspace.configuration
        
        logger.info(f"üåê Workspace: {current_workspace.name} ({current_workspace.root_path})")
        
    # @cn:function integration
    # @cn:process configuration-loading
    # @cn:step 2
    # @cn:file-dependency .contextrc
    # @cn:error-handling required

            
    # @cn:function integration
    # @cn:process context-maps-loading
    # @cn:step 3
    # @cn:directory-dependency .context-map/
    # @cn:depends-on configuration
    def _load_context_maps(self) -> None:
        """Carrega mapas de contexto existentes"""
        # Usar arquitetura workspace: mapas ficam em .cn_model/
        context_maps_path = self.output_dir
        map_files = ['index.yml', 'component-map.yml', 'context-map/index.yml']
        
        if not context_maps_path.exists():
            logger.warning("Mapas de contexto n√£o encontrados")
            return
            
        # Carregar mapas principais
        for map_file in map_files:
            file_path = context_maps_path / map_file
            if file_path.exists():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        map_name = map_file.replace('.yml', '')
                        self.context_maps[map_name] = yaml.safe_load(f)
                    logger.debug(f"Mapa carregado: {map_file}")
                except Exception as e:
                    logger.warning(f"Erro ao carregar {map_file}: {e}")
                    
    def _init_content_patterns(self) -> None:
        """Inicializa padr√µes para an√°lise de conte√∫do"""
        # Padr√µes para detec√ß√£o de tipo de documento
        self.template_patterns = {
            'decision': {
                'keywords': ['decis√£o', 'escolha', 'op√ß√£o', 'alternativa', 'pros', 'contras', 'justificativa', 'trade-off'],
                'structures': ['problema', 'an√°lise', 'op√ß√µes', 'decis√£o', 'consequ√™ncias'],
                'entities': ['ADR', 'RFC', 'PRD', 'escolha t√©cnica']
            },
            'process': {
                'keywords': ['processo', 'procedimento', 'passo', 'tutorial', 'guia', 'runbook', 'playbook'],
                'structures': ['objetivo', 'pr√©-requisitos', 'passos', 'valida√ß√£o', 'troubleshooting'],
                'entities': ['comando', 'execu√ß√£o', 'verifica√ß√£o', 'teste']
            },
            'reference': {
                'keywords': ['refer√™ncia', 'API', 'documenta√ß√£o', 'especifica√ß√£o', 'endpoint', 'par√¢metro'],
                'structures': ['overview', 'refer√™ncia', 'exemplos', 'recursos'],
                'entities': ['GET', 'POST', 'PUT', 'DELETE', 'HTTP', 'JSON', 'endpoint']
            },
            'architecture': {
                'keywords': ['arquitetura', 'design', 'componente', 'sistema', 'padr√£o', 'estrutura'],
                'structures': ['contexto', 'componentes', 'fluxos', 'decis√µes', 'evolu√ß√£o'],
                'entities': ['microservice', 'database', 'cache', 'queue', 'service']
            },
            'analysis': {
                'keywords': ['an√°lise', 'investiga√ß√£o', 'performance', 'bug', 'm√©trica', 'dados'],
                'structures': ['situa√ß√£o', 'an√°lise', 'descobertas', 'a√ß√µes', 'acompanhamento'],
                'entities': ['CPU', 'mem√≥ria', 'lat√™ncia', 'throughput', 'erro']
            },
            'planning': {
                'keywords': ['planejamento', 'roadmap', 'sprint', 'release', 'cronograma', 'marco'],
                'structures': ['objetivos', 'escopo', 'cronograma', 'riscos', 'm√©tricas'],
                'entities': ['milestone', 'deadline', 'resource', 'budget', 'timeline']
            }
        }
        
        # Padr√µes para detec√ß√£o de contexto
        self.context_patterns = {
            'infra': ['deploy', 'infrastructure', 'docker', 'kubernetes', 'aws', 'cloud'],
            'shared': ['utils', 'common', 'library', 'helper', 'shared', 'util'],
            'core': ['business', 'domain', 'logic', 'service', 'entity', 'model'],
            'api': ['endpoint', 'controller', 'route', 'rest', 'graphql', 'api'],
            'data': ['database', 'repository', 'model', 'migration', 'schema', 'query'],
            'ui': ['component', 'view', 'page', 'interface', 'frontend', 'react']
        }
        
    def analyze_content(self, content: str) -> ContentAnalysis:
        """
        Analisa conte√∫do de texto para extrair caracter√≠sticas
        
        Args:
            content: Conte√∫do a ser analisado
            
        Returns:
            An√°lise do conte√∫do
        """
        content_lower = content.lower()
        
        # Extrair palavras-chave
        keywords = []
        for template_type, patterns in self.template_patterns.items():
            for keyword in patterns['keywords']:
                if keyword in content_lower:
                    keywords.append(keyword)
                    
        # Extrair entidades t√©cnicas
        entities = []
        for template_type, patterns in self.template_patterns.items():
            for entity in patterns['entities']:
                if entity.lower() in content_lower:
                    entities.append(entity)
                    
        # Calcular score de complexidade baseado em indicadores
        complexity_indicators = [
            len(re.findall(r'```', content)),  # Blocos de c√≥digo
            len(re.findall(r'\|.*\|', content)),  # Tabelas
            len(re.findall(r'- \[', content)),  # Checklists
            len(re.findall(r'#{1,6}', content)),  # Headers
            content.count('http'),  # URLs
            content.count('@'),  # Refer√™ncias
        ]
        complexity_score = sum(complexity_indicators) / 100.0
        complexity_score = min(complexity_score, 1.0)
        
        # Determinar prop√≥sito principal
        purpose_scores = {}
        for template_type, patterns in self.template_patterns.items():
            score = 0
            for keyword in patterns['keywords']:
                score += content_lower.count(keyword)
            for entity in patterns['entities']:
                score += content_lower.count(entity.lower())
            purpose_scores[template_type] = score
            
        purpose = max(purpose_scores.keys(), key=lambda x: purpose_scores[x]) if purpose_scores else 'unknown'
        confidence = purpose_scores.get(purpose, 0) / 10.0
        confidence = min(confidence, 1.0)
        
        # Determinar dom√≠nio
        domain_scores = {}
        for context_type, keywords in self.context_patterns.items():
            score = sum(content_lower.count(keyword) for keyword in keywords)
            domain_scores[context_type] = score
            
        domain = max(domain_scores.keys(), key=lambda x: domain_scores[x]) if domain_scores else 'core'
        
        return ContentAnalysis(
            keywords=keywords[:10],  # Top 10 keywords
            entities=entities[:10],  # Top 10 entities
            complexity_score=complexity_score,
            purpose=purpose,
            domain=domain,
            confidence=confidence
        )
        
    def recommend_template(self, content: str, metadata: Optional[Dict[str, Any]] = None) -> TemplateRecommendation:
        """
        Recomenda template baseado no conte√∫do
        
        Args:
            content: Conte√∫do do documento
            metadata: Metadados existentes (opcional)
            
        Returns:
            Recomenda√ß√£o de template
        """
        analysis = self.analyze_content(content)
        
        # Se j√° tem doc_type nos metadados, validar
        if metadata and 'doc_type' in metadata:
            existing_type = metadata['doc_type']
            if existing_type in self.template_patterns:
                return TemplateRecommendation(
                    template_type=existing_type,
                    confidence=0.9,
                    reasoning=f"Tipo j√° definido nos metadados: {existing_type}",
                    alternative_templates=[]
                )
                
        # Calcular scores para cada template
        template_scores = {}
        
        for template_type, patterns in self.template_patterns.items():
            score = 0
            
            # Score baseado em keywords
            keyword_score = 0
            for keyword in patterns['keywords']:
                keyword_score += content.lower().count(keyword)
            score += keyword_score * 2
            
            # Score baseado em entidades
            entity_score = 0
            for entity in patterns['entities']:
                entity_score += content.lower().count(entity.lower())
            score += entity_score * 3
            
            # Score baseado em estruturas (headers)
            structure_score = 0
            for structure in patterns['structures']:
                if structure in content.lower():
                    structure_score += 1
            score += structure_score * 5
            
            template_scores[template_type] = score
            
        # Ordenar por score
        sorted_templates = sorted(template_scores.items(), key=lambda x: x[1], reverse=True)
        
        if not sorted_templates or sorted_templates[0][1] == 0:
            return TemplateRecommendation(
                template_type='decision',  # Default
                confidence=0.3,
                reasoning="N√£o foi poss√≠vel determinar tipo espec√≠fico, usando padr√£o DECIS√ÉO",
                alternative_templates=['process', 'reference']
            )
            
        best_template = sorted_templates[0][0]
        best_score = sorted_templates[0][1]
        
        # Calcular confian√ßa baseado no score
        confidence = min(best_score / 10.0, 1.0)
        
        # Templates alternativos
        alternatives = [t[0] for t in sorted_templates[1:3] if t[1] > 0]
        
        # Reasoning
        reasoning = f"An√°lise de conte√∫do indica tipo {best_template.upper()} "
        reasoning += f"(score: {best_score}, keywords: {len(analysis.keywords)}, "
        reasoning += f"entities: {len(analysis.entities)})"
        
        return TemplateRecommendation(
            template_type=best_template,
            confidence=confidence,
            reasoning=reasoning,
            alternative_templates=alternatives
        )
        
    def recommend_context(self, content: str, file_path: str, metadata: Optional[Dict[str, Any]] = None) -> ContextRecommendation:
        """
        Recomenda contexto baseado no conte√∫do e localiza√ß√£o
        
        Args:
            content: Conte√∫do do documento
            file_path: Caminho do arquivo
            metadata: Metadados existentes
            
        Returns:
            Recomenda√ß√£o de contexto
        """
        analysis = self.analyze_content(content)
        
        # Analisar path para determinar context_level
        path_parts = Path(file_path).parts
        
        context_level = 'c2_module'  # Default
        if len(path_parts) <= 2:
            context_level = 'c1_root'
        elif len(path_parts) >= 4:
            context_level = 'c3_component'
            
        # Determinar context_type baseado em an√°lise de conte√∫do
        context_type = analysis.domain
        
        # Sugerir m√≥dulo baseado no path e conte√∫do
        suggested_module = 'unknown'
        if len(path_parts) >= 2:
            suggested_module = path_parts[-2]  # Pasta pai
            
        # Se h√° metadados, validar consist√™ncia
        confidence = 0.7
        reasoning = f"Baseado em an√°lise de conte√∫do (dom√≠nio: {analysis.domain}) e estrutura de path"
        
        if metadata:
            if 'context_level' in metadata and metadata['context_level'] != context_level:
                confidence -= 0.2
                reasoning += f". ATEN√á√ÉO: Conflito com metadados existentes ({metadata['context_level']})"
                
        return ContextRecommendation(
            context_level=context_level,
            context_type=context_type,
            confidence=confidence,
            reasoning=reasoning,
            suggested_module=suggested_module
        )
        
    def suggest_connections(self, content: str, metadata: Dict[str, Any]) -> List[ConnectionSuggestion]:
        """
        Sugere conex√µes com outros documentos
        
        Args:
            content: Conte√∫do do documento
            metadata: Metadados do documento
            
        Returns:
            Lista de sugest√µes de conex√£o
        """
        suggestions = []
        
        if not self.context_maps.get('index'):
            return suggestions
            
        current_module = metadata.get('module', '')
        current_context_type = metadata.get('context_type', '')
        current_doc_type = metadata.get('doc_type', '')
        
        # Buscar documentos relacionados nos context maps
        document_summary = self.context_maps['index'].get('document_summary', {})
        
        for doc_path, doc_info in document_summary.items():
            if doc_path == metadata.get('path', ''):
                continue  # Skip self
                
            # Verificar se doc_info √© v√°lido
            if not doc_info or not isinstance(doc_info, dict):
                continue
                
            connection_score = 0
            connection_type = 'relates_to'
            reasoning_parts = []
            
            # Mesmo m√≥dulo = conex√£o mais forte
            if doc_info.get('module') == current_module:
                connection_score += 3
                reasoning_parts.append('mesmo m√≥dulo')
                
            # Mesmo context_type = conex√£o m√©dia
            if doc_info.get('context_type') == current_context_type:
                connection_score += 2
                reasoning_parts.append('mesmo contexto')
                
            # Padr√µes espec√≠ficos de depend√™ncia
            if current_doc_type == 'decision' and doc_info.get('type') == 'architecture':
                connection_score += 3
                connection_type = 'impacts'
                reasoning_parts.append('decis√£o impacta arquitetura')
                
            if current_doc_type == 'process' and doc_info.get('type') == 'reference':
                connection_score += 2
                connection_type = 'references'
                reasoning_parts.append('processo referencia documenta√ß√£o')
                
            if current_doc_type == 'analysis' and doc_info.get('type') == 'decision':
                connection_score += 3
                connection_type = 'impacts'
                reasoning_parts.append('an√°lise pode gerar decis√µes')
                
            # An√°lise de conte√∫do para refer√™ncias impl√≠citas
            doc_title = doc_info.get('title') or ''
            if doc_title:
                title_words = doc_title.lower().split()
                content_lower = content.lower()
                
                for word in title_words:
                    if len(word) > 3 and word in content_lower:
                        connection_score += 1
                        reasoning_parts.append(f"menciona '{word}'")
                        
            # S√≥ sugerir se score >= 2
            if connection_score >= 2:
                confidence = min(connection_score / 5.0, 1.0)
                reasoning = "Sugest√£o baseada em: " + ", ".join(reasoning_parts)
                
                suggestions.append(ConnectionSuggestion(
                    target_document=doc_path,
                    connection_type=connection_type,
                    confidence=confidence,
                    reasoning=reasoning
                ))
                
        # Ordenar por confian√ßa
        suggestions.sort(key=lambda x: x.confidence, reverse=True)
        
        return suggestions[:5]  # Top 5 sugest√µes
        
    def suggest_improvements(self, metadata: Dict[str, Any], content: str) -> List[ImprovementSuggestion]:
        """
        Sugere melhorias para o documento
        
        Args:
            metadata: Metadados do documento
            content: Conte√∫do do documento
            
        Returns:
            Lista de sugest√µes de melhoria
        """
        suggestions = []
        
        # Verificar metadados obrigat√≥rios
        required_fields = self.config.get('metadata', {}).get('required_fields', {})
        for field in required_fields:
            if field not in metadata or not metadata[field]:
                suggestions.append(ImprovementSuggestion(
                    type='metadata',
                    description=f"Campo obrigat√≥rio '{field}' n√£o preenchido",
                    priority='high',
                    action=f"Adicionar valor para '{field}' nos metadados"
                ))
                
        # Verificar conex√µes vazias
        connections = metadata.get('connections', {})
        if isinstance(connections, dict):
            empty_connections = [k for k, v in connections.items() if not v]
            if len(empty_connections) == len(connections):
                suggestions.append(ImprovementSuggestion(
                    type='connections',
                    description="Documento n√£o tem conex√µes mapeadas",
                    priority='medium',
                    action="Revisar e mapear relacionamentos com outros documentos"
                ))
                
        # Verificar qualidade do conte√∫do
        analysis = self.analyze_content(content)
        
        if analysis.complexity_score < 0.1:
            suggestions.append(ImprovementSuggestion(
                type='content',
                description="Conte√∫do parece muito simples ou incompleto",
                priority='medium',
                action="Adicionar mais detalhes, exemplos ou estrutura"
            ))
            
        if len(analysis.keywords) < 3:
            suggestions.append(ImprovementSuggestion(
                type='content',
                description="Poucos termos t√©cnicos identificados",
                priority='low',
                action="Considerar adicionar mais contexto t√©cnico espec√≠fico"
            ))
            
        # Verificar data de atualiza√ß√£o
        last_updated = metadata.get('last_updated')
        if last_updated:
            try:
                update_date = datetime.fromisoformat(last_updated)
                days_old = (datetime.now() - update_date).days
                if days_old > 90:
                    suggestions.append(ImprovementSuggestion(
                        type='maintenance',
                        description=f"Documento n√£o atualizado h√° {days_old} dias",
                        priority='medium',
                        action="Revisar e atualizar informa√ß√µes se necess√°rio"
                    ))
            except ValueError:
                suggestions.append(ImprovementSuggestion(
                    type='metadata',
                    description="Formato de data inv√°lido em 'last_updated'",
                    priority='high',
                    action="Corrigir formato da data para YYYY-MM-DD"
                ))
                
        # Verificar se template est√° sendo usado corretamente
        template_rec = self.recommend_template(content, metadata)
        current_type = metadata.get('doc_type')
        
        if current_type and current_type != template_rec.template_type and template_rec.confidence > 0.7:
            suggestions.append(ImprovementSuggestion(
                type='template',
                description=f"Tipo '{current_type}' pode n√£o ser ideal. Sugest√£o: '{template_rec.template_type}'",
                priority='medium',
                action=f"Considerar usar template {template_rec.template_type.upper()}"
            ))
            
        return suggestions
        
    def detect_patterns(self) -> Dict[str, Any]:
        """
        Detecta padr√µes nos documentos existentes
        
        Returns:
            Dicion√°rio com padr√µes detectados
        """
        patterns = {
            'usage_patterns': {},
            'context_patterns': {},
            'connection_patterns': {},
            'quality_patterns': {}
        }
        
        if not self.context_maps.get('index'):
            return patterns
            
        document_summary = self.context_maps['index'].get('document_summary', {})
        
        # Padr√µes de uso de templates
        type_distribution = self.context_maps['index'].get('type_distribution', {})
        total_docs = sum(type_distribution.values())
        
        for doc_type, count in type_distribution.items():
            percentage = (count / total_docs) * 100 if total_docs > 0 else 0
            patterns['usage_patterns'][doc_type] = {
                'count': count,
                'percentage': round(percentage, 1),
                'trend': 'normal'  # Poderia calcular tend√™ncia temporal
            }
            
        # Padr√µes de contexto
        context_distribution = self.context_maps['index'].get('context_distribution', {})
        for context_level, count in context_distribution.items():
            percentage = (count / total_docs) * 100 if total_docs > 0 else 0
            patterns['context_patterns'][context_level] = {
                'count': count,
                'percentage': round(percentage, 1)
            }
            
        # Padr√µes de conex√£o
        if self.context_maps.get('connections'):
            connections_data = self.context_maps['connections']
            patterns['connection_patterns'] = {
                'strong_coupling_count': len(connections_data.get('strong_coupling', [])),
                'weak_coupling_count': len(connections_data.get('weak_coupling', [])),
                'isolated_count': len(connections_data.get('isolated_components', [])),
                'connection_types': connections_data.get('connection_types', {})
            }
            
        # Padr√µes de qualidade
        docs_with_errors = sum(1 for doc in document_summary.values() if doc.get('has_errors', False))
        patterns['quality_patterns'] = {
            'error_rate': round((docs_with_errors / total_docs) * 100, 1) if total_docs > 0 else 0,
            'total_documents': total_docs,
            'documents_with_errors': docs_with_errors
        }
        
        return patterns
        
    def analyze_document(self, file_path: str, content: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        An√°lise completa de um documento
        
        Args:
            file_path: Caminho do arquivo
            content: Conte√∫do do documento
            metadata: Metadados existentes
            
        Returns:
            An√°lise completa com recomenda√ß√µes
        """
        content_analysis = self.analyze_content(content)
        template_rec = self.recommend_template(content, metadata)
        context_rec = self.recommend_context(content, file_path, metadata)
        connection_suggestions = self.suggest_connections(content, metadata)
        improvement_suggestions = self.suggest_improvements(metadata, content)
        
        return {
            'file_path': file_path,
            'content_analysis': {
                'keywords': content_analysis.keywords,
                'entities': content_analysis.entities,
                'complexity_score': content_analysis.complexity_score,
                'purpose': content_analysis.purpose,
                'domain': content_analysis.domain,
                'confidence': content_analysis.confidence
            },
            'template_recommendation': {
                'recommended_type': template_rec.template_type,
                'confidence': template_rec.confidence,
                'reasoning': template_rec.reasoning,
                'alternatives': template_rec.alternative_templates
            },
            'context_recommendation': {
                'context_level': context_rec.context_level,
                'context_type': context_rec.context_type,
                'suggested_module': context_rec.suggested_module,
                'confidence': context_rec.confidence,
                'reasoning': context_rec.reasoning
            },
            'connection_suggestions': [
                {
                    'target': suggestion.target_document,
                    'type': suggestion.connection_type,
                    'confidence': suggestion.confidence,
                    'reasoning': suggestion.reasoning
                }
                for suggestion in connection_suggestions
            ],
            'improvement_suggestions': [
                {
                    'type': suggestion.type,
                    'description': suggestion.description,
                    'priority': suggestion.priority,
                    'action': suggestion.action
                }
                for suggestion in improvement_suggestions
            ],
            'overall_score': self._calculate_overall_score(
                content_analysis, template_rec, context_rec, improvement_suggestions
            )
        }
        
    def _calculate_overall_score(self, content_analysis: ContentAnalysis, 
                               template_rec: TemplateRecommendation,
                               context_rec: ContextRecommendation,
                               improvements: List[ImprovementSuggestion]) -> float:
        """
        Calcula score geral do documento
        
        Returns:
            Score de 0.0 a 1.0
        """
        score = 0.0
        
        # Score baseado em confian√ßa das recomenda√ß√µes
        score += template_rec.confidence * 0.3
        score += context_rec.confidence * 0.2
        score += content_analysis.confidence * 0.2
        
        # Penalizar por melhorias necess√°rias
        high_priority_improvements = len([i for i in improvements if i.priority == 'high'])
        medium_priority_improvements = len([i for i in improvements if i.priority == 'medium'])
        
        penalty = (high_priority_improvements * 0.1) + (medium_priority_improvements * 0.05)
        score = max(0.0, score - penalty)
        
        # Bonus por complexidade adequada
        if 0.3 <= content_analysis.complexity_score <= 0.8:
            score += 0.1
            
        return min(score, 1.0)

def main():
    """Fun√ß√£o principal para teste da engine"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Context Navigator Engine')
    parser.add_argument('--path', '-p', default='.', 
                       help='Caminho base do projeto')
    parser.add_argument('--analyze', '-a', 
                       help='Analisar arquivo espec√≠fico')
    parser.add_argument('--patterns', action='store_true',
                       help='Detectar padr√µes gerais')
    
    args = parser.parse_args()
    
    engine = ContextEngine(args.path)
    
    if args.patterns:
        patterns = engine.detect_patterns()
        print("\n=== PADR√ïES DETECTADOS ===")
        print(json.dumps(patterns, indent=2, ensure_ascii=False))
        
    if args.analyze:
        file_path = Path(args.analyze)
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # Extrair metadados (simplificado)
            metadata = {}
            if content.startswith('---'):
                parts = content.split('---', 2)
                if len(parts) >= 3:
                    try:
                        metadata = yaml.safe_load(parts[1]) or {}
                    except:
                        pass
                        
            analysis = engine.analyze_document(str(file_path), content, metadata)
            print(f"\n=== AN√ÅLISE DE {file_path} ===")
            print(json.dumps(analysis, indent=2, ensure_ascii=False))
        else:
            print(f"Arquivo n√£o encontrado: {file_path}")

if __name__ == '__main__':
    main() 