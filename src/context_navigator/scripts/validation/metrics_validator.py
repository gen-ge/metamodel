#!/usr/bin/env python3
"""
Context Navigator - Metrics Validator

Avalia o sistema atual contra as m√©tricas de sucesso definidas no PRD.
"""

import json
import yaml
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import sys
import argparse

class MetricsValidator:
    def __init__(self, config_path: str = ".contextrc"):
        # NOVO: Usar WorkspaceManager para detectar workspace
        self._init_with_workspace_manager()
        
        # M√©tricas do PRD
        self.target_metrics = {
            "ai_consistency": 95.0,  # 95%+ prompts seguem context.rule
            "documentation_coverage": 90.0,  # 90%+ documentos com metadados completos
            "automatic_detection": 95.0,  # 95%+ contextos detectados corretamente
            "conflict_resolution": 90.0,  # 90%+ conflitos detectados automaticamente
            "efficiency_reduction": 80.0,  # 80% redu√ß√£o em retrabalho documental
            "context_switch_time": 2.0,  # <2 minutos para recuperar contexto
            "connection_precision": 95.0,  # 95%+ conex√µes corretas
            "template_completeness": 100.0,  # 100% casos cobertos por templates
            "manual_intervention": 0.0  # 0 interven√ß√£o manual regular
        }
        
    def _init_with_workspace_manager(self):
        """Inicializa usando WorkspaceManager para detectar workspace"""
        # Importar WorkspaceManager
        try:
            from ..core.workspace_manager import WorkspaceManager, Workspace
        except ImportError:
            # Fallback para desenvolvimento
            import sys
            sys.path.insert(0, str(Path(__file__).parent.parent))
            from core.workspace_manager import WorkspaceManager, Workspace
        
        # Detectar workspace atual
        workspace_manager = WorkspaceManager()
        current_workspace = workspace_manager.detect_current_workspace()
        
        if not current_workspace:
            print("‚ùå Context Navigator workspace n√£o encontrado")
            print("üí° Execute 'cn init' para configurar este diret√≥rio")
            sys.exit(1)
        
        # Configurar paths baseado no workspace
        self.workspace = current_workspace
        self.base_path = current_workspace.root_path
        self.output_dir = current_workspace.root_path / ".cn_model"
        
        # Configura√ß√£o vem do workspace
        self.config = current_workspace.configuration
        
        # Paths para nova arquitetura
        self.context_map_path = self.output_dir
        self.templates_path = self._get_global_templates_path()
        self.docs_path = self.output_dir / "docs"
        self.examples_path = self.base_path / "examples"
        self.scripts_path = Path(__file__).parent
        
        print(f"üåê Workspace: {current_workspace.name} ({current_workspace.root_path})")
        
    def _get_global_templates_path(self) -> Path:
        """Detecta onde est√£o os templates na instala√ß√£o global"""
        possible_locations = [
            Path("/opt/context-navigator/templates"),
            Path.home() / ".local" / "share" / "context-navigator" / "templates",
        ]
        
        for location in possible_locations:
            if location.exists():
                return location
        
        # Fallback para desenvolvimento
        return Path(__file__).parent.parent / "templates"
        
    def load_context_map(self, filename: str) -> Dict:
        """Carrega mapa de contexto espec√≠fico"""
        map_path = self.context_map_path / filename
        if not map_path.exists():
            return {}
        
        try:
            with open(map_path, 'r', encoding='utf-8') as file:
                return yaml.safe_load(file)
        except yaml.YAMLError:
            # Se houver erro YAML, tenta uma abordagem mais simples
            if filename == "conflicts.yml":
                return self.parse_conflicts_manually(map_path)
            return {}
    
    def parse_conflicts_manually(self, conflicts_path: Path) -> Dict:
        """Parse manual do arquivo de conflitos se YAML falhar"""
        try:
            with open(conflicts_path, 'r', encoding='utf-8') as file:
                content = file.read()
            
            # Extrai informa√ß√µes b√°sicas usando regex
            total_conflicts = content.count('- component:')
            duplicate_refs = content.count('duplicate_references')
            orphaned_docs = content.count('orphaned_document')
            
            return {
                "summary": {
                    "total_conflicts": total_conflicts,
                    "by_type": {
                        "duplicate_references": duplicate_refs,
                        "orphaned_document": orphaned_docs
                    }
                }
            }
        except Exception:
            return {}
    
    def evaluate_ai_consistency(self) -> Dict[str, Any]:
        """Avalia consist√™ncia da IA com context.rule"""
        score = 0.0
        details = []
        
        # Verificar se context.rule existe
        if Path("context.rule").exists():
            score += 30.0
            details.append("‚úÖ context.rule existe")
        else:
            details.append("‚ùå context.rule n√£o encontrado")
        
        # Verificar tamanho e completude do context.rule
        if Path("context.rule").exists():
            with open("context.rule", 'r', encoding='utf-8') as f:
                content = f.read()
                if len(content) > 5000:  # Conte√∫do substancial
                    score += 20.0
                    details.append("‚úÖ context.rule tem conte√∫do substancial")
                else:
                    details.append("‚ö†Ô∏è context.rule pode ser mais detalhado")
        
        # Verificar manuais de IA
        if (self.docs_path / "MANUAL_IA.md").exists():
            score += 25.0
            details.append("‚úÖ Manual da IA existe")
        else:
            details.append("‚ùå Manual da IA n√£o encontrado")
        
        # Verificar conven√ß√µes
        if (self.docs_path / "CONVENTIONS.md").exists():
            score += 25.0
            details.append("‚úÖ Conven√ß√µes definidas")
        else:
            details.append("‚ùå Conven√ß√µes n√£o definidas")
        
        return {
            "score": score,
            "target": self.target_metrics["ai_consistency"],
            "status": "‚úÖ PASS" if score >= self.target_metrics["ai_consistency"] else "‚ùå FAIL",
            "details": details
        }
    
    def evaluate_documentation_coverage(self) -> Dict[str, Any]:
        """Avalia cobertura documental com metadados"""
        index_map = self.load_context_map("index.yml")
        
        if not index_map or "document_summary" not in index_map:
            return {
                "score": 0.0,
                "target": self.target_metrics["documentation_coverage"],
                "status": "‚ùå FAIL",
                "details": ["‚ùå Nenhum documento encontrado"]
            }
        
        documents = index_map["document_summary"]
        total_docs = len(documents)
        docs_with_metadata = 0
        details = []
        
        for doc_path, doc_info in documents.items():
            if doc_info.get("type") and doc_info.get("context_level") and doc_info.get("context_type"):
                docs_with_metadata += 1
                details.append(f"‚úÖ {doc_path} - metadados completos")
            else:
                details.append(f"‚ùå {doc_path} - metadados incompletos")
        
        score = (docs_with_metadata / total_docs) * 100 if total_docs > 0 else 0.0
        
        return {
            "score": score,
            "target": self.target_metrics["documentation_coverage"],
            "status": "‚úÖ PASS" if score >= self.target_metrics["documentation_coverage"] else "‚ùå FAIL",
            "details": details,
            "stats": {
                "total_documents": total_docs,
                "with_metadata": docs_with_metadata,
                "coverage_percentage": score
            }
        }
    
    def evaluate_automatic_detection(self) -> Dict[str, Any]:
        """Avalia detec√ß√£o autom√°tica de contextos"""
        index_map = self.load_context_map("index.yml")
        
        if not index_map or "context_distribution" not in index_map:
            return {
                "score": 0.0,
                "target": self.target_metrics["automatic_detection"],
                "status": "‚ùå FAIL",
                "details": ["‚ùå Mapas de contexto n√£o encontrados"]
            }
        
        context_dist = index_map["context_distribution"]
        total_docs = sum(context_dist.values())
        unknown_docs = context_dist.get("unknown", 0)
        detected_docs = total_docs - unknown_docs
        
        score = (detected_docs / total_docs) * 100 if total_docs > 0 else 0.0
        
        details = []
        for context, count in context_dist.items():
            if context == "unknown":
                details.append(f"‚ùå {context}: {count} documentos")
            else:
                details.append(f"‚úÖ {context}: {count} documentos")
        
        return {
            "score": score,
            "target": self.target_metrics["automatic_detection"],
            "status": "‚úÖ PASS" if score >= self.target_metrics["automatic_detection"] else "‚ùå FAIL",
            "details": details,
            "stats": {
                "total_documents": total_docs,
                "detected_documents": detected_docs,
                "unknown_documents": unknown_docs,
                "detection_percentage": score
            }
        }
    
    def evaluate_conflict_resolution(self) -> Dict[str, Any]:
        """Avalia resolu√ß√£o de conflitos"""
        conflicts_map = self.load_context_map("conflicts.yml")
        
        if not conflicts_map or "summary" not in conflicts_map:
            return {
                "score": 100.0,  # Sem conflitos = 100%
                "target": self.target_metrics["conflict_resolution"],
                "status": "‚úÖ PASS",
                "details": ["‚úÖ Nenhum conflito detectado"]
            }
        
        summary = conflicts_map["summary"]
        total_conflicts = summary.get("total_conflicts", 0)
        
        details = []
        if total_conflicts == 0:
            score = 100.0
            details.append("‚úÖ Nenhum conflito detectado")
        else:
            # Analisa tipos de conflitos
            by_type = summary.get("by_type", {})
            critical_conflicts = by_type.get("orphaned_document", 0) + by_type.get("broken_connections", 0)
            warning_conflicts = by_type.get("duplicate_references", 0) + by_type.get("circular_dependencies", 0)
            
            # Score baseado na criticidade
            resolution_score = max(0, 100 - (critical_conflicts * 20) - (warning_conflicts * 5))
            score = resolution_score
            
            for conflict_type, count in by_type.items():
                severity = "cr√≠tico" if conflict_type in ["orphaned_document", "broken_connections"] else "aviso"
                details.append(f"‚ö†Ô∏è {conflict_type}: {count} ({severity})")
        
        return {
            "score": score,
            "target": self.target_metrics["conflict_resolution"],
            "status": "‚úÖ PASS" if score >= self.target_metrics["conflict_resolution"] else "‚ùå FAIL",
            "details": details,
            "stats": {
                "total_conflicts": total_conflicts,
                "resolution_score": score
            }
        }
    
    def evaluate_efficiency(self) -> Dict[str, Any]:
        """Avalia efici√™ncia (baseado em indicadores indiretos)"""
        # M√©tricas indiretas de efici√™ncia
        score = 0.0
        details = []
        
        # Templates dispon√≠veis
        templates = list(self.templates_path.glob("*.md"))
        if len(templates) >= 6:
            score += 20.0
            details.append(f"‚úÖ {len(templates)} templates dispon√≠veis")
        else:
            details.append(f"‚ö†Ô∏è Apenas {len(templates)} templates dispon√≠veis")
        
        # Scripts de automa√ß√£o
        scripts = list(self.scripts_path.glob("*.py"))
        if len(scripts) >= 4:
            score += 20.0
            details.append(f"‚úÖ {len(scripts)} scripts de automa√ß√£o")
        else:
            details.append(f"‚ö†Ô∏è Apenas {len(scripts)} scripts de automa√ß√£o")
        
        # Documenta√ß√£o completa
        required_docs = ["MANUAL_HUMANO.md", "MANUAL_IA.md", "CONVENTIONS.md"]
        existing_docs = sum(1 for doc in required_docs if (self.docs_path / doc).exists())
        if existing_docs == len(required_docs):
            score += 20.0
            details.append("‚úÖ Documenta√ß√£o completa")
        else:
            details.append(f"‚ö†Ô∏è Documenta√ß√£o incompleta ({existing_docs}/{len(required_docs)})")
        
        # Mapas de contexto autom√°ticos
        context_maps = list(self.context_map_path.glob("*.yml")) + list(self.context_map_path.glob("*.json"))
        if len(context_maps) >= 5:
            score += 20.0
            details.append(f"‚úÖ {len(context_maps)} mapas de contexto")
        else:
            details.append(f"‚ö†Ô∏è Apenas {len(context_maps)} mapas de contexto")
        
        # Conex√µes entre documentos
        connections_map = self.load_context_map("connections.yml")
        if connections_map and len(connections_map) > 0:
            score += 20.0
            details.append("‚úÖ Conex√µes entre documentos mapeadas")
        else:
            details.append("‚ö†Ô∏è Conex√µes n√£o mapeadas")
        
        return {
            "score": score,
            "target": self.target_metrics["efficiency_reduction"],
            "status": "‚úÖ PASS" if score >= self.target_metrics["efficiency_reduction"] else "‚ùå FAIL",
            "details": details
        }
    
    def evaluate_context_switch_time(self) -> Dict[str, Any]:
        """Avalia tempo de mudan√ßa de contexto"""
        # M√©tricas indiretas baseadas em estrutura
        score = 0.0
        details = []
        
        # Index centralizado
        if (self.context_map_path / "index.yml").exists():
            score += 40.0
            details.append("‚úÖ √çndice centralizado dispon√≠vel")
        else:
            details.append("‚ùå √çndice centralizado n√£o encontrado")
        
        # Context rule para IA
        if Path("context.rule").exists():
            score += 30.0
            details.append("‚úÖ Context rule para IA dispon√≠vel")
        else:
            details.append("‚ùå Context rule n√£o encontrado")
        
        # Navega√ß√£o por conex√µes
        connections_map = self.load_context_map("connections.yml")
        if connections_map and len(connections_map) > 0:
            score += 30.0
            details.append("‚úÖ Navega√ß√£o por conex√µes dispon√≠vel")
        else:
            details.append("‚ùå Navega√ß√£o por conex√µes n√£o dispon√≠vel")
        
        # Assume <2 minutos se score alto
        estimated_time = 2.0 - (score / 100.0 * 1.5)  # Linear entre 2.0 e 0.5 minutos
        
        return {
            "score": score,
            "target": self.target_metrics["context_switch_time"],
            "status": "‚úÖ PASS" if estimated_time <= self.target_metrics["context_switch_time"] else "‚ùå FAIL",
            "details": details,
            "stats": {
                "estimated_time_minutes": estimated_time,
                "infrastructure_score": score
            }
        }
    
    def evaluate_connection_precision(self) -> Dict[str, Any]:
        """Avalia precis√£o das conex√µes"""
        connections_map = self.load_context_map("connections.yml")
        conflicts_map = self.load_context_map("conflicts.yml")
        
        if not connections_map:
            return {
                "score": 0.0,
                "target": self.target_metrics["connection_precision"],
                "status": "‚ùå FAIL",
                "details": ["‚ùå Nenhuma conex√£o encontrada"]
            }
        
        total_connections = 0
        broken_connections = 0
        details = []
        
        # Conta conex√µes totais
        for doc_path, connections in connections_map.items():
            if isinstance(connections, dict):
                for conn_type, conn_list in connections.items():
                    if isinstance(conn_list, list):
                        total_connections += len(conn_list)
        
        # Conta conex√µes quebradas dos conflitos
        if conflicts_map and "conflicts" in conflicts_map:
            for conflict in conflicts_map["conflicts"]:
                if conflict.get("type") == "broken_connections":
                    broken_connections += 1
        
        if total_connections == 0:
            score = 0.0
            details.append("‚ùå Nenhuma conex√£o encontrada")
        else:
            valid_connections = total_connections - broken_connections
            score = (valid_connections / total_connections) * 100
            details.append(f"‚úÖ {valid_connections}/{total_connections} conex√µes v√°lidas")
            if broken_connections > 0:
                details.append(f"‚ö†Ô∏è {broken_connections} conex√µes quebradas")
        
        return {
            "score": score,
            "target": self.target_metrics["connection_precision"],
            "status": "‚úÖ PASS" if score >= self.target_metrics["connection_precision"] else "‚ùå FAIL",
            "details": details,
            "stats": {
                "total_connections": total_connections,
                "valid_connections": total_connections - broken_connections,
                "broken_connections": broken_connections,
                "precision_percentage": score
            }
        }
    
    def evaluate_template_completeness(self) -> Dict[str, Any]:
        """Avalia completude dos templates"""
        required_templates = ["decisao.md", "processo.md", "referencia.md", 
                            "arquitetura.md", "analise.md", "planejamento.md"]
        
        existing_templates = []
        missing_templates = []
        details = []
        
        for template in required_templates:
            template_path = self.templates_path / template
            if template_path.exists():
                existing_templates.append(template)
                details.append(f"‚úÖ {template} dispon√≠vel")
            else:
                missing_templates.append(template)
                details.append(f"‚ùå {template} n√£o encontrado")
        
        score = (len(existing_templates) / len(required_templates)) * 100
        
        return {
            "score": score,
            "target": self.target_metrics["template_completeness"],
            "status": "‚úÖ PASS" if score >= self.target_metrics["template_completeness"] else "‚ùå FAIL",
            "details": details,
            "stats": {
                "required_templates": len(required_templates),
                "existing_templates": len(existing_templates),
                "missing_templates": len(missing_templates),
                "completeness_percentage": score
            }
        }
    
    def evaluate_manual_intervention(self) -> Dict[str, Any]:
        """Avalia necessidade de interven√ß√£o manual"""
        # Baseado em automa√ß√£o dispon√≠vel
        score = 0.0
        details = []
        
        # Scanner autom√°tico
        if (self.scripts_path / "context_scanner.py").exists():
            score += 25.0
            details.append("‚úÖ Scanner autom√°tico dispon√≠vel")
        else:
            details.append("‚ùå Scanner autom√°tico n√£o encontrado")
        
        # Validador autom√°tico
        if (self.scripts_path / "template_validator.py").exists():
            score += 25.0
            details.append("‚úÖ Validador autom√°tico dispon√≠vel")
        else:
            details.append("‚ùå Validador autom√°tico n√£o encontrado")
        
        # Detector de conflitos
        if (self.scripts_path / "conflict_detector.py").exists():
            score += 25.0
            details.append("‚úÖ Detector de conflitos dispon√≠vel")
        else:
            details.append("‚ùå Detector de conflitos n√£o encontrado")
        
        # Engine de contexto
        if (self.scripts_path / "context_engine.py").exists():
            score += 25.0
            details.append("‚úÖ Engine de contexto dispon√≠vel")
        else:
            details.append("‚ùå Engine de contexto n√£o encontrado")
        
        # Inverte a pontua√ß√£o (maior automa√ß√£o = menor interven√ß√£o manual)
        intervention_score = 100 - score
        
        return {
            "score": score,
            "target": self.target_metrics["manual_intervention"],
            "status": "‚úÖ PASS" if intervention_score <= self.target_metrics["manual_intervention"] else "‚ùå FAIL",
            "details": details,
            "stats": {
                "automation_score": score,
                "intervention_needed": intervention_score
            }
        }
    
    def run_full_evaluation(self) -> Dict[str, Any]:
        """Executa avalia√ß√£o completa de todas as m√©tricas"""
        print("üîç Iniciando avalia√ß√£o completa das m√©tricas...")
        
        evaluations = {
            "ai_consistency": self.evaluate_ai_consistency(),
            "documentation_coverage": self.evaluate_documentation_coverage(),
            "automatic_detection": self.evaluate_automatic_detection(),
            "conflict_resolution": self.evaluate_conflict_resolution(),
            "efficiency": self.evaluate_efficiency(),
            "context_switch_time": self.evaluate_context_switch_time(),
            "connection_precision": self.evaluate_connection_precision(),
            "template_completeness": self.evaluate_template_completeness(),
            "manual_intervention": self.evaluate_manual_intervention()
        }
        
        # Calcula score geral
        total_score = 0
        passed_metrics = 0
        
        for metric_name, result in evaluations.items():
            if result["status"] == "‚úÖ PASS":
                passed_metrics += 1
            total_score += result["score"]
        
        overall_score = total_score / len(evaluations)
        
        return {
            "timestamp": datetime.now().isoformat(),
            "overall_score": overall_score,
            "passed_metrics": passed_metrics,
            "total_metrics": len(evaluations),
            "pass_rate": (passed_metrics / len(evaluations)) * 100,
            "evaluations": evaluations
        }
    
    def print_report(self, evaluation_result: Dict[str, Any]):
        """Imprime relat√≥rio detalhado"""
        print("\n" + "="*80)
        print("üìä RELAT√ìRIO DE M√âTRICAS - CONTEXT NAVIGATOR")
        print("="*80)
        
        print(f"\nüéØ SCORE GERAL: {evaluation_result['overall_score']:.1f}%")
        print(f"‚úÖ M√âTRICAS APROVADAS: {evaluation_result['passed_metrics']}/{evaluation_result['total_metrics']}")
        print(f"üìà TAXA DE APROVA√á√ÉO: {evaluation_result['pass_rate']:.1f}%")
        
        if evaluation_result['pass_rate'] >= 80:
            print("üèÜ EXCELENTE - Sistema atende √†s especifica√ß√µes do PRD!")
        elif evaluation_result['pass_rate'] >= 60:
            print("‚úÖ BOM - Sistema atende √† maioria das especifica√ß√µes")
        else:
            print("‚ö†Ô∏è ATEN√á√ÉO - Sistema precisa de melhorias")
        
        print("\nüìã AVALIA√á√ÉO DETALHADA:")
        print("-" * 80)
        
        for metric_name, result in evaluation_result['evaluations'].items():
            print(f"\nüîç {metric_name.upper().replace('_', ' ')}")
            print(f"   Score: {result['score']:.1f}% (Meta: {result['target']:.1f}%)")
            print(f"   Status: {result['status']}")
            
            if result['details']:
                print("   Detalhes:")
                for detail in result['details'][:5]:  # Limita a 5 detalhes
                    print(f"     - {detail}")
                if len(result['details']) > 5:
                    print(f"     ... e mais {len(result['details']) - 5} itens")
        
        print("\n" + "="*80)
        print(f"‚è∞ Relat√≥rio gerado em: {evaluation_result['timestamp']}")
        print("="*80)

def main():
    parser = argparse.ArgumentParser(description='Context Navigator - Validador de M√©tricas')
    parser.add_argument('--config', default='.contextrc', help='Caminho para configura√ß√£o (ignorado na v2.0)')
    parser.add_argument('--save', help='Salvar resultado em arquivo JSON')
    
    args = parser.parse_args()
    
    try:
        # Detectar workspace usando WorkspaceManager
        try:
            from ..core.workspace_manager import WorkspaceManager
        except ImportError:
            # Fallback para desenvolvimento
            import sys
            sys.path.insert(0, str(Path(__file__).parent.parent))
            from core.workspace_manager import WorkspaceManager
        
        workspace_manager = WorkspaceManager()
        current_workspace = workspace_manager.detect_current_workspace()
        
        if not current_workspace:
            print("‚ùå Context Navigator workspace n√£o encontrado")
            print("üí° Execute 'cn init' para configurar este diret√≥rio")
            return 1
        
        validator = MetricsValidator()
        result = validator.run_full_evaluation()
        validator.print_report(result)
        
        if args.save:
            with open(args.save, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            print(f"\nüíæ Resultado salvo em: {args.save}")
        
    except Exception as e:
        print(f"‚ùå Erro durante avalia√ß√£o: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main() 